# AudioContext

AudioContext 代表由每一个 AudioNode 链接的音频模块构建的音频处理图。  
audio context 不仅可以控制它内部的节点创建，也可以控制 音频解码和音频处理的执行。 
所有的音频处理操作都在这个 context 内部。  

audioContext 用于管理和播放所有的声音。
合成声音：创建一个或多个 sound source 并且 connect 到 sound destination. 
这种连接可以是不直接的，中间可以有一些音频信号处理节点。
这里有一个类似的 pipeline 的概念。      


```js
var audioContext = new AudioContext();

// 创建一个源节点，可以通过 audio 元素的方式，也可以通过 arrayBuffer 的方式
var sourceNode = audioContext.createMediaElementSource(audioEle);

// 创建一个目标节点
var targetNode = audioContext.createMediaStreamDestination();

// 将源节点和目标节点链接，然后就可以从目标节点取东西
sourceNode.connect(targetNode);

// 可以从 目标节点 上取出流数据
var astream = targetNode.stream;


// 这个节点是耳机节点
audioContext.destination
// 如果没有以下代码，耳机不会输出声音
sourceNode.connect(audioContext.destination);

```

## bufferLoader
```js
function BufferLoader(context, urlList, callback) {
  this.context = context;
  this.urlList = urlList;
  this.onload = callback;
  this.bufferList = new Array();
  this.loadCount = 0;
}

BufferLoader.prototype.loadBuffer = function(url, index) {
  // Load buffer asynchronously
  var request = new XMLHttpRequest();
  request.open("GET", url, true);
  request.responseType = "arraybuffer";

  var loader = this;

  request.onload = function() {
    // Asynchronously decode the audio file data in request.response
    loader.context.decodeAudioData(
      request.response,
      function(buffer) {
        if (!buffer) {
          alert('error decoding file data: ' + url);
          return;
        }
        loader.bufferList[index] = buffer;
        if (++loader.loadCount == loader.urlList.length)
          loader.onload(loader.bufferList);
      },
      function(error) {
        console.error('decodeAudioData error', error);
      }
    );
  }

  request.onerror = function() {
    alert('BufferLoader: XHR error');
  }

  request.send();
}

BufferLoader.prototype.load = function() {
  for (var i = 0; i < this.urlList.length; ++i)
  this.loadBuffer(this.urlList[i], i);
}

```




