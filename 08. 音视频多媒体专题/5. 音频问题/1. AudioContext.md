# AudioContext

AudioContext 代表由每一个 AudioNode 链接的音频模块构建的音频处理图。  
audio context 不仅可以控制它内部的节点创建，也可以控制 音频解码和音频处理的执行。 
所有的音频处理操作都在这个 context 内部。  

audioContext 用于管理和播放所有的声音。
合成声音：创建一个或多个 sound source 并且 connect 到 sound destination. 
这种连接可以是不直接的，中间可以有一些音频信号处理节点。
这里有一个类似的 pipeline 的概念, 可以多个并行。      


```js
var audioContext = new AudioContext();

// 创建一个源节点，可以通过 audio 元素的方式，也可以通过 arrayBuffer 的方式
var sourceNode = audioContext.createMediaElementSource(audioEle);

// 创建一个目标节点
var targetNode = audioContext.createMediaStreamDestination();

// 将源节点和目标节点链接，然后就可以从目标节点取东西
sourceNode.connect(targetNode);

// 可以从 目标节点 上取出流数据
var astream = targetNode.stream;


// 这个节点是耳机节点
audioContext.destination
// 如果没有以下代码，耳机不会输出声音
sourceNode.connect(audioContext.destination);

```


# AudioNode 节点
每个节点都可以使用 两种方法 创建：1. 构造函数， 2. audioCtx.create 工厂方法，第一个参数是 audioCtx. 

## 1. source 节点
根据已有源 创建 source 节点的三种方法：
* audioCtx.createMediaElementSource(ele): 传入 audio/video 元素   
* audioCtx.createMediaStreamSource(stream): 传入 stream, 可以将用户的输入作为输入       
* audioCtx.createBufferSource()；  source.buffer = xxx;   
第三种方法需要把整个 audio 的 buffer 下载完了以后，才能传入。  

创建 OscillatorNode 振荡器节点，自己产生声音源。 
```js
const oscillator = new OscillatorNode(audioCtx);

// 或
const oscillator2 = audioCtx.createOscillator();
```

#### 1.1 source 节点的用法
start/ stop / resume;   

#### 1.2 oscillator 节点的用法


## 2. audioCtx.destination
这个节点指的是 耳机/音箱 节点，如果不链接的话，听不到声音     

## 3. dest 节点
```js
const destNode = audioContext.createMediaStreamDestination();
// 将 dest 节点的 stream 导出
const outputStream = destNode.stream;
```
这个节点的作用是为了 合成输出。  


## 4. 音量控制节点 GainNode（音量控制）

## 5. 处理节点： BiquadFilterNode, ConvolverNode, PannerNode




## 6. AnalyserNode (音频的可视化)
AnalyserNode 用于获取音频的频率数据（ FrequencyData ）和时域数据（ TimeDomainData ）。从而实现音频的可视化。
它只会对音频进行读取，而不会对音频进行任何改变。
```js
const analyser = ctx.createAnalyser();
analyser.fftSize = 512;
```
关于 fftSize ，在 MDN 上的介绍可能很难理解，说是快速傅里叶变换的一个参数。

可以从以下角度理解：

1. 它的取值是什么？

fftSize 的要求是 2 的幂次方，比如 256 、 512 等。数字越大，得到的结果越精细。

对于移动端网页来说，本身音频的比特率大多是 128Kbps ，没有必要用太大的频率数组去存储本身就不够精细的源数据。另外，手机屏幕的尺寸比桌面端小，因此最终展示图形也不需要每个频率都采到。只需要体现节奏即可，因此 512 是较为合理的值。

2. 它的作用是什么？

fftSize 决定了 frequencyData 的长度，具体为 fftSize 的一半。

至于为什么是 1 / 2，感兴趣的可以看下这篇文章: 
https://dsp.stackexchange.com/questions/4825/why-is-the-fft-mirrored


```js
analyser.getByteFrequencyData   //  返回的是 0 - 255 的 Uint8Array
analyser.getFloatFrequencyData  //  返回的是 0 - 22050 的 Float32Array 
```
两者都是返回 TypedArray ，唯一的区别是精度不同。  
相比较而言，如果项目中对性能的要求高于精度，那建议使用 getByteFrequencyData 

```js
const bufferLength = analyser.frequencyBinCount;
const dataArray = new Uint8Array(bufferLength);
analyser.getByteFrequencyData(dataArray);
```



