# VS Code 中的 Text Buffer 的重新实现 [译]

原文标题： Text Buffer Reimplementation
原文链接： https://code.visualstudio.com/blogs/2018/03/23/text-buffer-reimplementation
原文作者： Peng Lyu ( VS Code Team member)  

正文开始

在 VS Code 的 1.21 发布版本中包含了一项重大改进：全新的 text buffer 实现，在内存和速度方面都有大幅的性能提升。在这篇文章里，我会告诉大家：我们是如何选择和设计数据结构和算法，从而达成这些改进的。

在关于 javascript 程序的性能讨论中，经常会有一种声音：建议使用 native code 实现。针对 VS Code 的 text buffer 问题，这些讨论早在一年多以前就开始了。 在经历深度探索之后，我们发现用 c++ 实现 text buffer 的确会使内存大幅度下降，但是并没有得到我们期望的性能提升。在 native 和 V8 引擎之间字符串转换非常消耗性能，消耗了通过 c++ 实现带来的性能优势。 在文章的末尾，我们会详细讨论这个问题。    

既然不能采用 native 的方式，所以我们必须找到方法来优化 javasript/typescript 代码。有一些非常有启发性的文章在讲如何优化 js， 例如 Vyacheslav Egorov 的这篇文章里[Maybe you don't need Rust and WASM to speed up your JS](https://mrale.ph/blog/2018/02/03/maybe-you-dont-need-rust-to-speed-up-your-js.html)，展示了如何把 javascript 引擎推到极限，从而榨取尽可能多的性能。即使不使用这些偏底层的引擎优化技巧，通过使用更合适的数据结构和更快的算法来把速度提升一个或几个数量级，仍然是可能的。  


# 之前的 text buffer 数据结构
编辑器的核心模型是基于文本行的，例如，开发者是一行一行地读写代码，编译器提供的运行时诊断/堆栈追踪提供了行数和列数，tokenization 引擎是一行一行地运行，等等。虽然简单，我们希望通过 text buffer 的实现来增强 VS Code 的想法， 从我们第一天启动 Monaco Editor 项目时就没有变过。 开始的时候，我们使用的是以行为单位的数组的方法，并且运行良好，因为常规的文本输入都相对较小。当用户正在输入时，我们在数组中定位到用户正在输入的行，并且修改这一行的字符串进行替换；当用户插入新的一行时，我们在行数组( line array) 中插入一个新的行对象，由 JS 引擎帮我们完成繁重的底层内存操作。

但是，我们在不断地收到 VS Code 的崩溃反馈：当打开某些文件的时候会导致内存不足。例如，用户打开一个 35 MB 的文件失败。问题原因在于，这个文件里有太多的行，1370万行。我们之前为每一行创建了一个 ModelLine 对象，每个对象大概占用了 40 - 60 字节，所以 line array 使用了大约 600 MB 的内存来存放文档。这个内存大小是文件原始大小的 20 倍。  

line array 表示方法的另外一个问题就是打开一个文件的速度。 为了构建 line array，我必须把内容按行进行拆分，每一行一个字符串对象。这个拆分行为消耗大量性能，你会在下面的基准测试（benchmark） 中看到这一点。


# 新的 text buffer 实现
line array 表示方法占用了大量内存，并且在构建的时候非常耗时，但它的好处是能够快速进行行查找。理想的情况下，我们希望只存储文件的文本，没有额外的 metadata。 所以，我们开始寻找一种数据结构，占用尽可能少的 metadata。在评估完一些数据结构以后，我发现 piece table 可能是一个很好的备选项。     

## 使用 piece table 来避免过多的 meta-data
Piece Table 是一种用于表示文本文档中一系列编辑的数据结构：
```ts
class PieceTable {
  original: string; // original contents
  added: string; // user added contents
  nodes: Node[];
}

class Node {
  type: NodeType;
  start: number;
  length: number;
}

enum NodeType {
  Original,
  Added
}
```
在文件加载完之后，piece table 将整个文件内容存储在 `original` field 中， 这个时候 `added` field 是空的, table 里只有一个类型为`NodeType.Original`的 Node。 当用户在文件尾部输入时，我们将新增加的内容添加到 `added` field, 并且在 node list 中插入一个类型为 `NodeType.Added` 的 Node。类似地，如果用户是在文件中间输入时，我们将原来的 Node 进行拆分，并且根据需要插入一个新的 Node。

下面的动画展示了如何在 piece table 中以一行一行的方式访问文档行。它有两个 buffer (original 和 added), 三个 Node（在原来的内容中间插入了一段文字）。

[插图1](traditional-piece-table.gif)

piece table 的初始内存大小，十分接近原始文件的大小，编辑动作需要的内容正比于编辑动作的数量和新增文本的大小。所以，一般来说，piece table 在内存方面具有巨大的优势。 但是，低内存的代价就是，访问一个行（真实的行）的速度慢。例如，如果你想要得到第 1000 行的内容，唯一的办法是从文档的开始的每一个字符开始遍历，从第 999 个换行符开始，读取字符，直到下一个换行符号。  


## 使用 caching 进行更快的行查找
传统的 piece table 的 nodes 中只包含了偏移量，但是我们可以在里面添加换行信息，从而可以更快的行查找。直观的方法就是，在 node 节点的文本中找到每一个换行符的 offset，并且存储起来。 
```ts
class PieceTable {
  original: string;
  added: string;
  nodes: Node[];
}

class Node {
  type: NodeType;
  start: number;
  length: number;
  lineStarts: number[];   // 存储换行符的 offset
}

enum NodeType {
  Original,
  Added
}
```
例如，如果你想从一个给定的 Node 中找到第二行，你可以从 `node.lineStarts[0]` and `node.lineStarts[1]` 的相对偏移量中读取到这一行的文本的位置。因为我们直到一个 Node 中有多少个换行，所以访问一个文档中随机行变得很直接：从第一个 Node （而不是之前的字符）开始读取，直到找到目标点的换行符。

这个算法仍然很简单，但是相比之前，已经很好了，我们可以直接跳过文本中的 chunk，而不是之前的一个字符一个字符的查找。我们会在下面讨论，如何可以做的更好。    

## 避免字符串拼接陷阱

piece table 包含有个两个 buffer，一个用于存放是原始文件内容，另外一个用于存放用户编辑。 在 VS Code 中，我们通过 node.js 的 fs.readFile 来以 64KB 的 chunks 来加载文件。 所以，当文件很大时，例如 64MB，我们会得到 1000 个 chunks。 在收到所有的 chunks 以后，我们把它们拼接为一个巨大的字符串，存放到 piece table 的 original buffer 中。  

这看起来是可行合理的，但是事实上，V8 引擎会无情打你的脸。我尝试过打开一个 500 MB 的文件 但是得到了一个异常，因为在我使用的 V8 引擎的版本中，字符串的最大长度是 256 MB。 这个限制会在 V8 引擎的未来版本中提高到 1 GB，但是并不是从根本上解决问题。    


相比原来的 piece table 中只有两个 buffer （original buffer 和 added buffer） 而言， 我们改成了 buffers list， 尽量保证这个 list 不能过长，通过 fs.readFile 的时候避免字符串拼接。 每次收到 64KB 的 chunk，我们会把它放进 buffer list 中，并且创建一个 Node 指向这个 buffer。  
```ts
class PieceTable {
  buffers: string[];      // buffer list
  nodes: Node[];
}

class Node {
  bufferIndex: number;      // buffer index
  start: number; // start offset in buffers[bufferIndex]
  length: number;
  lineStarts: number[];
}
```

## 使用平衡二叉树(红黑树)来加速行查找

不使用字符串拼接的方法的情况下，我们现在打开大文件，可能会产生潜在的性能问题。比如，打开一个 64MB 的文件，piece table 会有 1000 个节点。即使我们在每个节点中缓存了换行符位置，我们也不会知道哪一个绝对行是对应哪一个节点。 为了获取某一行的内容，我们需要从第一个节点开始遍历，直到找到该行。对于最坏的 case，这个时间复杂度是 O(n), n 表示的节点的数量。   

在每个 node 中缓存绝对的行数（第几行），并且在 list 上使用 二分查找法，可以加快查找速度。 但是，当我们修改一个节点时，必须要遍历后面所有的节点，进行行数更新。 这是不合适的，但是二分查找法的思路是对的。 为了能够达到同样的效果，我们可以使用平衡二叉树。    

我们现在必须决定使用什么 metadata 来作为比较树节点的关键。正如前面提到的，使用node 在文档中的 offset 或 绝对行数会使得编辑动作的时间复杂度为 O(N). 如果我们想要让时间复杂度降为 O(log N), 我们需要（树）节点和和的它的子节点的相对信息。所以，当用户编辑文本时，我们重新计算修改该节点的 metadata，并且把变化一直冒泡到树的根节点。 

如果一个节点只有4个属性（bufferIndex, start, length, lineStarts），会花费数秒才能找结果。 为了更快地找到结果，我们可以把左节点的 text length 和 line starts 也存储在节点中。通过这种方法，从根节点开始通过 offset 或 line number 搜索，可以非常高效。 存储右节点也能达到同样的效果，但是我们不需要两个都存。

改造以后的结构如下：
```ts
class PieceTable {
  buffers: string[];
  rootNode: Node;       // 红黑树的根节点
}

class Node {
  bufferIndex: number;
  start: number;
  length: number;
  lineStarts: number[];

  left_subtree_length: number;    // 左节点的 text length
  left_subtree_lfcnt: number;     // 左节点的 换行符 数量

  left: Node;
  right: Node;
  parent: Node;
}
```

[插图2](piece-tree.gif)

## Piece Tree
我更喜欢称这种 text buffer 为：使用红黑树的多 buffer piece table， 用于优化 line model。 但是在我们日常的站例会中，每个人的发言必须要在 90 秒内完成，多次重复这么长的句子是不明智的，所以我就简单称它为 piece tree. 

理论上理解这种数据结构是一回事，在实际中的性能又是另外一回事。我们使用的开发语言，代码运行的环境，使用方调用API的方式，还有其他因素都可能会影响到结果。 基准测试(Benchmarks)提供了一个全面综合性的评估结果， 所以我们用同样的小/中/大文件，同时在line array 和 piece tree 两种版本下，运行基准测试。


## 基准测试
测试文件：
1. checker.ts:  1.46 MB, 26 K 行
2. sqlite.c:    4.31 MB, 128 K 行
3. Russian English Bilingual dictionary: 14 MB, 552 K 行  
手动创建的更大的文件：
4. Chromium heap snapshot of newly opened VS Code Insider： 54 MB， 3 M 行
5. checker.ts X 128 - 184MB, 3M lines

### 1. 内存使用情况

The memory usage of the piece tree immediately after loading is very close to the original file size, and it is significantly lower than the old implementation. First round, piece tree wins:

[插图3]()

### 2. 打开文件耗时
Finding and caching line breaks is much faster than splitting the file into an array of strings:

[插图4]()


### 3. 编辑性能
I have simulated two workflows:

Making edits in random positions in the document.
Typing in sequence.
I tried to mimic these two scenarios: Apply 1000 random edits or 1000 sequential inserts to the document, then see how much time every text buffer needs:

[插图5]()

As expected, line array wins when the file is very small. Accessing a random position in a small array and tweaking a string which has around 100~150 characters is really fast. The line array starts to choke when the file has many lines (100k+). Sequential inserts in large files make this situation worse as the JavaScript engine does a lot of work in order to resize the large array. Piece tree behaves in a stable fashion as each edit is just a string append and a couple red-black tree operations.


### 4. 读取性能
For our text buffers, the hottest method is getLineContent. It is invoked by the view code, by the tokenizer, the link detector, and pretty much every component relying on document content. Some of the code traverses the entire file, like the link detector, while other code reads only a window of sequential lines, like the view code. So I set out to benchmark this method in various scenarios:

Call getLineContent for all lines after doing 1000 random edits.
Call getLineContent for all lines after doing 1000 sequential inserts.
Read 10 distinct line windows after doing 1000 random edits.
Read 10 distinct line windows after doing 1000 sequential inserts.



[插图5]()


TA DA, we found the Achilles heel of piece tree. A large file, with 1000s of edits, will lead to thousands or tens of thousands of nodes. Even though looking up a line is O(log N), where N is the number of nodes, that is significantly more than O(1) which the line array enjoyed.

Having thousands of edits is relatively rare. You might get there after replacing a commonly occurring sequence of characters in a large file. Also, we are talking about microseconds for each getLineContent call so it is not something we are concerned about at this time. Most of getLineContent calls are from view rendering and tokenization, and the post processes of line contents are much more time consuming. DOM construction and rendering or tokenization of a view port usually takes tens of milliseconds, in which getLineContent only accounts for less than 1%. Nevertheless, we are considering eventually implementing a normalization step, where we would recreate buffers and nodes if certain conditions such as a high number of nodes are met.


# 结论









